{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Classify Images based on *master category* label from *styles* dataframe**"
      ],
      "metadata": {
        "id": "Mxp0th54rlOx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load Libraries**"
      ],
      "metadata": {
        "id": "AzP2GnVBb08P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras"
      ],
      "metadata": {
        "id": "_F52aL9Zbdsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "NQ3dMSXL-Wyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pillow # https://pypi.org/project/pillow/"
      ],
      "metadata": {
        "id": "m4agTnCwjbIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python"
      ],
      "metadata": {
        "id": "gJGJfIRX-xXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import cv2  # OpenCV\n",
        "from PIL import Image  # Pillow\n",
        "from skimage.feature import hog, local_binary_pattern\n",
        "import random\n",
        "import os\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "mDww_Am5ZjVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **EDA on Styles Data**"
      ],
      "metadata": {
        "id": "iNpE4Zm9btXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load labels of the images from styles.csv into a dataframe\n",
        "df = pd.read_csv('data/styles.csv')"
      ],
      "metadata": {
        "id": "MFZVQlIrMSgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 'id' in this dataframe represents the name of the image file (ex.49543.jpg; The 49543 is the id of the image in the dataframe)\n",
        "df.sample(5)"
      ],
      "metadata": {
        "id": "Mr19a2SvMaXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "p8ahGuzjPRmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing values in Dataframe by percentage\n",
        "df_missing = df.isnull().sum()/df.shape[0]*100\n",
        "df_missing = df_missing.sort_values(ascending=False)\n",
        "df_missing\n",
        "#df_missing.plot.hist()"
      ],
      "metadata": {
        "id": "OBjI4MwWP0Tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove missing values even though the missing values percentage is very low and does not affect the classification\n",
        "df = df.dropna(axis=0)"
      ],
      "metadata": {
        "id": "hFDQVLM4QA8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "kwS3a08zRs4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataFrame of Master Category Data Distribution\n",
        "df_master_category = df.groupby(['masterCategory'])['masterCategory'].count().reset_index(name='count')\n",
        "df_master_category"
      ],
      "metadata": {
        "id": "AA4-pNmbdCJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Graphical representation of Master Category Data Distribution\n",
        "plt.figure(figsize=(15,6))\n",
        "barplot = sns.barplot(x='masterCategory', y='count', hue= 'masterCategory', data=df_master_category)\n",
        "plt.title('Master Category Data Distribution')\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "# Add count annotations\n",
        "for bar in barplot.patches:\n",
        "    height = bar.get_height()\n",
        "    barplot.annotate(f'{int(height)}',  # Display as an integer\n",
        "                     xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                     xytext=(0, 5),  # Offset position slightly above the bar\n",
        "                     textcoords=\"offset points\",\n",
        "                     ha='center', va='bottom')  # Centered horizontally\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1bnTz_PBePDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataFrame of Sub Category Data Distribution\n",
        "df_sub_category = df.groupby(['subCategory'])['subCategory'].count().reset_index(name='count')\n",
        "df_sub_category"
      ],
      "metadata": {
        "id": "IYWH6rb1d5oG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Graphical representation of Sub Category Data Distribution\n",
        "plt.figure(figsize=(20,6))\n",
        "barplot = sns.barplot(x='subCategory', y='count', hue='subCategory', data=df_sub_category)\n",
        "plt.title('Sub Category Data Distribution')\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "# Add count annotations\n",
        "for bar in barplot.patches:\n",
        "    height = bar.get_height()\n",
        "    barplot.annotate(f'{int(height)}',  # Display as an integer\n",
        "                     xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                     xytext=(0, 5),  # Offset position slightly above the bar\n",
        "                     textcoords=\"offset points\",\n",
        "                     ha='center', va='bottom')  # Centered horizontally\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Jwh_szF3fz8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataFrame of Article Type Data Distribution\n",
        "df_articleType = df.groupby(['articleType'])['articleType'].count().reset_index(name='count')\n",
        "df_articleType"
      ],
      "metadata": {
        "id": "aF3_WETXeE0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Graphical representation of Article Type Data Distribution\n",
        "plt.figure(figsize=(40,6))\n",
        "barplot = sns.barplot(x='articleType', y='count', hue= 'articleType',data=df_articleType)\n",
        "plt.title('Article Type Data Distribution')\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "# Add count annotations\n",
        "for bar in barplot.patches:\n",
        "    height = bar.get_height()\n",
        "    barplot.annotate(f'{int(height)}',  # Display as an integer\n",
        "                     xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                     xytext=(0, 5),  # Offset position slightly above the bar\n",
        "                     textcoords=\"offset points\",\n",
        "                     ha='center', va='bottom')  # Centered horizontally\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DufCZgCFheAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataframe: colour Distribution by Gender\n",
        "df_gender_colour = df.groupby(['gender','baseColour'])['gender'].count().reset_index(name='count')\n",
        "df_gender_colour"
      ],
      "metadata": {
        "id": "ekZwNfWLT2NY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Graphical representation of colour distribution by Gender\n",
        "df_gender_colour_sorted = df_gender_colour.sort_values(by='count', ascending=False)\n",
        "\n",
        "# Create the barplot\n",
        "plt.figure(figsize=(25, 6))\n",
        "df_gender_colour_sorted['log_count'] = np.log10(df_gender_colour_sorted['count'])\n",
        "sns.lineplot(x='baseColour', y='log_count', hue='gender', data=df_gender_colour_sorted)\n",
        "plt.title('Colour Distribution by Gender (Sorted by Preference)')\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g0bsh3QkV5u4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Images colour distribution by Master Category\n",
        "df_master_category = df.groupby(['masterCategory','baseColour'])['masterCategory'].count().reset_index(name='count')\n",
        "df_master_category"
      ],
      "metadata": {
        "id": "LLkUx2Rhbl_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Images colour distribution by Master Category\n",
        "df_master_category_sorted = df_master_category.sort_values(by='count', ascending=False)\n",
        "\n",
        "# Create the barplot\n",
        "plt.figure(figsize=(25, 6))\n",
        "df_master_category_sorted['log_count'] = np.log10(df_master_category_sorted['count'])\n",
        "sns.barplot(x='baseColour', y='log_count', hue='masterCategory', data=df_master_category_sorted)\n",
        "plt.title('Colour Distribution by Master Category (Sorted by Preference)')\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fk9FCak5b1yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Images colour distribution by usage\n",
        "df_usage = df.groupby(['usage','baseColour'])['usage'].count().reset_index(name='count')\n",
        "df_usage"
      ],
      "metadata": {
        "id": "yeRY5dGTezoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Graphical representation of colour distribution by usage\n",
        "\n",
        "# Create the heatmap\n",
        "plt.figure(figsize=(35, 6))\n",
        "df_usage['log_count'] = np.log10(df_usage['count'])\n",
        "# Pass arguments as keywords to pivot\n",
        "pivot_table = df_usage.pivot(index='usage', columns='baseColour', values='log_count')\n",
        "sns.heatmap(pivot_table, annot=True, fmt=\".0f\", cmap=\"YlGnBu\")\n",
        "plt.title('Colour Distribution Heatmap by Usage')\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o4iGgfKue-YQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **EDA on Images Data**"
      ],
      "metadata": {
        "id": "GQ9VRaNSigia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display sample pictures\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "# Function to display sample images\n",
        "def display_sample_images(image_folder, num_samples=5):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    for i, filename in enumerate(os.listdir(image_folder)[:num_samples]):\n",
        "        img_path = os.path.join(image_folder, filename)\n",
        "        img = load_img(img_path)\n",
        "        plt.subplot(1, num_samples, i + 1)\n",
        "        plt.imshow(img)\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Display sample images\n",
        "display_sample_images('images', num_samples=10)\n"
      ],
      "metadata": {
        "id": "QYNtxhcFi1Tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot colour distribution of a sample image\n",
        "def plot_color_distribution(image_folder, sample_image_filename):\n",
        "    img_path = os.path.join(image_folder, sample_image_filename)\n",
        "    img = img_to_array(load_img(img_path))\n",
        "\n",
        "    # Display the image\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.imshow(img/255.) # Display the image (normalized for display)\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Sample Image: {sample_image_filename}\")\n",
        "    plt.show()\n",
        "\n",
        "    # Plot color distribution\n",
        "    colors = ('r', 'g', 'b')\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    for i, color in enumerate(colors):\n",
        "        plt.hist(img[:, :, i].ravel(), bins=20, color=color, alpha=0.5, label=color.upper())\n",
        "    plt.title('Color Distribution')\n",
        "    plt.xlabel('Pixel Value')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Plot color distribution for a sample image\n",
        "image_files = os.listdir('images')\n",
        "sample_image_filename = image_files[3]  # Take the first image as a sample\n",
        "plot_color_distribution('images', sample_image_filename)  # Pass the filename to the function"
      ],
      "metadata": {
        "id": "Itj6OBi1i_QK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pre-processing of data: Images and Labels**"
      ],
      "metadata": {
        "id": "nz7bqqN-c7qf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "# Load images from the images folder\n",
        "def load_images_from_folder(folder, img_size=(224, 224), df=None):  # Pass the DataFrame to the function\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    for subdir, dirs, files in os.walk(folder):\n",
        "        for file in files:\n",
        "            try:\n",
        "                # Extract image ID from filename (assuming filename format: id_*.jpg)\n",
        "                image_id = int(file.split('.')[0])\n",
        "\n",
        "                # Get the corresponding 'masterCategory' from the DataFrame\n",
        "                label_series = df.loc[df['id'] == image_id, 'masterCategory']\n",
        "                if not label_series.empty:  # Check if the series is not empty\n",
        "                    label = label_series.values[0]\n",
        "                else:\n",
        "                    print(f\"Warning: No label found for image ID {image_id} in file {file}. Skipping this image.\")\n",
        "                    continue  # Skip this image and go to the next one\n",
        "\n",
        "                img_path = os.path.join(subdir, file)\n",
        "                img = load_img(img_path, target_size=img_size)\n",
        "                img_array = img_to_array(img)\n",
        "                images.append(img_array)\n",
        "                labels.append(label)\n",
        "            except ValueError:\n",
        "                 print(f\"Warning: Unable to process file {file}. Is the file in the correct format? Skipping this image.\")\n",
        "                 continue\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Error loading or processing {file}. Error: {e}. Skipping this image.\")\n",
        "                continue\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# Load the dataframe (make sure this matches where the actual file is located)\n",
        "df = pd.read_csv('data/styles.csv')\n",
        "\n",
        "# Load images, passing the DataFrame to get labels\n",
        "folder = \"images\"\n",
        "X, y = load_images_from_folder(folder, df=df)  # Pass the DataFrame here\n",
        "\n",
        "# Perform train-test split using the new 'y'\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)\n",
        "\n",
        "# Normalize image data\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "print(f\"Training data shape: {X_train.shape}\")\n",
        "print(f\"Testing data shape: {X_test.shape}\")"
      ],
      "metadata": {
        "id": "0bJZVPSBEpdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Sampling few images\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(10):\n",
        "    plt.subplot(1, 10, i + 1)\n",
        "    plt.xticks()\n",
        "    plt.yticks()\n",
        "    plt.grid(False)\n",
        "#    plt.imshow(X_train[i], cmap=plt.cm.binary)\n",
        "    plt.imshow(X_train[i], cmap=plt.cm.magma)\n",
        "    plt.xlabel(y_train[i])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VR-CIWH7epyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Modeling**"
      ],
      "metadata": {
        "id": "xLstnuJ_egAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate_model function:\n",
        "def evaluate_model(model, X_test, y_test, model_name):\n",
        "   y_pred = model.predict(X_test)\n",
        "   accuracy = accuracy_score(y_test, y_pred)\n",
        "   print(f\"{model_name} Accuracy: {accuracy}\")\n",
        "   print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # Confusion Matrix Visualization (Seaborn)\n",
        "   cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    # Get unique class names from y_test and y_pred\n",
        "   class_names_unique = np.unique(np.concatenate((y_test, y_pred)))\n",
        "\n",
        "   cm_df = pd.DataFrame(cm, index=class_names_unique, columns=class_names_unique)  # Use unique class names\n",
        "   print(f\"{model_name} Confusion Matrix:\")\n",
        "   print(cm_df)"
      ],
      "metadata": {
        "id": "oa7rrQJMe-zX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model1(model, X_test, y_test, model_name):\n",
        "    # Predict probabilities and get predicted class labels\n",
        "    y_pred_probs = model.predict(X_test)\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)  # Get the class with highest probability\n",
        "\n",
        "    # If using label encoding, inverse transform to original labels\n",
        "    if isinstance(model, keras.Sequential):  # Assuming CNN model is a keras.Sequential\n",
        "        # Get the label encoder\n",
        "        label_encoder = globals().get('label_encoder')\n",
        "        if label_encoder:\n",
        "            y_pred = label_encoder.inverse_transform(y_pred)  # Convert back to original labels\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"{model_name} Accuracy: {accuracy}\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # Confusion Matrix Visualization (Seaborn)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    # Get unique class names from y_test and y_pred\n",
        "    class_names_unique = np.unique(np.concatenate((y_test, y_pred)))\n",
        "\n",
        "    cm_df = pd.DataFrame(cm, index=class_names_unique, columns=class_names_unique)  # Use unique class names\n",
        "    print(f\"{model_name} Confusion Matrix:\")\n",
        "    print(cm_df)"
      ],
      "metadata": {
        "id": "rys8PJA74MQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model1: SVM model\n",
        " # SVM with Raw Images (Reshaped)\n",
        "# Reshape images for SVM (flatten)\n",
        "x_train_reshaped = X_train.reshape(X_train.shape[0], -1)\n",
        "x_test_reshaped = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "svm_images = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
        "svm_images.fit(x_train_reshaped, y_train)"
      ],
      "metadata": {
        "id": "-lOK45KOfGBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model1 (SVM performance)\n",
        "\n",
        "evaluate_model(svm_images, x_test_reshaped, y_test, \"SVM with Raw Images\")"
      ],
      "metadata": {
        "id": "VbBJXoJif5kJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predictions from the trained model\n",
        "y_pred = svm_images.predict(x_test_reshaped)"
      ],
      "metadata": {
        "id": "JXyC9bH4xxuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder # Import LabelEncoder\n",
        "# Assuming y_train contains your original training labels\n",
        "label_encoder = LabelEncoder()  # Create a LabelEncoder instance\n",
        "label_encoder.fit(y_train)  # Fit the encoder to your training labels\n",
        "\n",
        "confusionMatrix = confusion_matrix(y_test, y_pred)\n",
        "cmd = ConfusionMatrixDisplay(confusion_matrix=confusionMatrix, display_labels=label_encoder.classes_)\n",
        "plt.figure(figsize=(20, 20))\n",
        "cmd.plot()\n",
        "plt.title(\"SVC Confusion Matrix\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.yticks(rotation=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8iX5CFUZ7n9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model2: KNN Model\n",
        "# KNN classifier\n",
        "# Train a KNN model\n",
        "knn_model = KNeighborsClassifier(n_neighbors=3)\n",
        "knn_model.fit(x_train_reshaped, y_train)"
      ],
      "metadata": {
        "id": "HK7b30pngHeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evalauate model2 (KNN performance)\n",
        "evaluate_model(knn_model, x_test_reshaped, y_test, \"KNN model evaluation\")"
      ],
      "metadata": {
        "id": "XdAMLeWYgiP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predictions from the trained model\n",
        "y_pred = knn_model.predict(x_test_reshaped)"
      ],
      "metadata": {
        "id": "_5vaz9i80VtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder # Import LabelEncoder\n",
        "# Assuming y_train contains your original training labels\n",
        "label_encoder = LabelEncoder()  # Create a LabelEncoder instance\n",
        "label_encoder.fit(y_train)  # Fit the encoder to your training labels\n",
        "\n",
        "confusionMatrix = confusion_matrix(y_test, y_pred)\n",
        "cmd = ConfusionMatrixDisplay(confusion_matrix=confusionMatrix, display_labels=label_encoder.classes_)\n",
        "plt.figure(figsize=(20, 20))\n",
        "cmd.plot()\n",
        "plt.title(\"KNN Confusion Matrix\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.yticks(rotation=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pBskdenI7kMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model3: RandomForest model\n",
        "# RandomForest classifier\n",
        "# Train a RandomForest model\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(x_train_reshaped, y_train)"
      ],
      "metadata": {
        "id": "iPGfmOtPgwxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evalauate model3 (RandomForest performance)\n",
        "# Evalauate RandomForest model\n",
        "evaluate_model(rf_model, x_test_reshaped, y_test, \"RandomForest model evaluation\")"
      ],
      "metadata": {
        "id": "KNJkv_hEhBgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predictions from the trained model\n",
        "y_pred = rf_model.predict(x_test_reshaped)"
      ],
      "metadata": {
        "id": "Y8ELtX9w0tpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder # Import LabelEncoder\n",
        "# Assuming y_train contains your original training labels\n",
        "label_encoder = LabelEncoder()  # Create a LabelEncoder instance\n",
        "label_encoder.fit(y_train)  # Fit the encoder to your training labels\n",
        "\n",
        "confusionMatrix = confusion_matrix(y_test, y_pred)\n",
        "cmd = ConfusionMatrixDisplay(confusion_matrix=confusionMatrix, display_labels=label_encoder.classes_)\n",
        "plt.figure(figsize=(20, 20))\n",
        "cmd.plot()\n",
        "plt.title(\"RandomForest Confusion Matrix\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.yticks(rotation=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fc3xca6t7PLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **HyperParameter Tuning and GridSearachCV**"
      ],
      "metadata": {
        "id": "aVvFYPilhjrF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The step execution takes long time atleast 20 minutes for 500+ images, so plan accordingly to run.**"
      ],
      "metadata": {
        "id": "sacKOSI9t31R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Only SVC, KNN, RandormForest are being selected. GradientBoosting was excluded as it is taking lot longer time to train the images.\n",
        "# Define the parameter grid for each model\n",
        "param_grid = {\n",
        "    'SVC': {\n",
        "        'kernel': ['linear', 'rbf', 'poly'],\n",
        "        'C': [0.1, 1, 10],\n",
        "        'gamma': ['scale', 'auto']\n",
        "    },\n",
        "    'KNN': {\n",
        "        'n_neighbors': [3, 5, 7],\n",
        "        'weights': ['uniform', 'distance'],\n",
        "        'metric': ['euclidean', 'manhattan']\n",
        "    },\n",
        "    'RandomForest': {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_depth': [None, 10, 20],\n",
        "        'min_samples_split': [2, 5, 10]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Create a dictionary to store the models\n",
        "models = {\n",
        "    'SVC': SVC(),\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'RandomForest': RandomForestClassifier()\n",
        "}\n",
        "\n",
        "# Create a dictionary to store the best models and parameters\n",
        "best_models = {}\n",
        "\n",
        "# Perform GridSearchCV for each model\n",
        "for model_name, model in models.items():\n",
        "    grid_search = GridSearchCV(model, param_grid[model_name], cv=5, scoring='accuracy')\n",
        "    grid_search.fit(x_train_reshaped, y_train)  # Use your training data here\n",
        "\n",
        "    best_models[model_name] = {\n",
        "        'model': grid_search.best_estimator_,\n",
        "        'params': grid_search.best_params_,\n",
        "        'score': grid_search.best_score_\n",
        "    }\n",
        "\n",
        "# Print the best model and parameters for each algorithm\n",
        "for model_name, results in best_models.items():\n",
        "    print(f\"Best {model_name}:\")\n",
        "    print(f\"  Parameters: {results['params']}\")\n",
        "    print(f\"  Score: {results['score']}\")\n",
        "\n",
        "\n",
        "# Evaluate the best models on the test set\n",
        "for model_name, results in best_models.items():\n",
        "  evaluate_model(results['model'], x_test_reshaped, y_test, f\"Best {model_name}\")\n"
      ],
      "metadata": {
        "id": "C9allTDhhlt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **OpenCV** - Model performance evaluation using OpenCV, HOG, LBP via features extraction and SVC"
      ],
      "metadata": {
        "id": "mnV9o59AiAZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Feature Extraction (Pillow/OpenCV, with Texture)\n",
        "def extract_features(images):\n",
        "     # Initialize an empty list to store features\n",
        "    features = []\n",
        "    for image in images:\n",
        "\n",
        "        # Color Histogram (using OpenCV)\n",
        "        if len(image.shape) == 3:  # If the image has 3 channels (RGB/BGR)\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "        image = image.astype(np.uint8)\n",
        "        hist = cv2.calcHist([image], [0], None, [256], [0, 256])  # Correct syntax for calcHist\n",
        "        hist = hist.flatten()\n",
        "\n",
        "        # HOG (Histogram of Oriented Gradients) - Texture\n",
        "        hog_features, hog_image = hog(image, orientations=8, pixels_per_cell=(4, 4),\n",
        "                                      cells_per_block=(2, 2), visualize=True)\n",
        "\n",
        "        # LBP (Local Binary Patterns) - Texture\n",
        "        radius = 1\n",
        "        n_points = 8 * radius\n",
        "        lbp = local_binary_pattern(image, n_points, radius, method='uniform')\n",
        "        lbp_hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, n_points + 2), range=(0, n_points + 1))\n",
        "        lbp_hist = lbp_hist.astype(\"float\")\n",
        "        lbp_hist /= (lbp_hist.sum() + 1e-7)\n",
        "\n",
        "        # Combine all features into a single array\n",
        "        image_features = np.concatenate([hist, hog_features, lbp_hist])\n",
        "        features.append(image_features)\n",
        "\n",
        "    return np.array(features)"
      ],
      "metadata": {
        "id": "JuhaApkBfWSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add feature for the Xtrain and Xtest\n",
        "\n",
        "x_train_features = extract_features(X_train)\n",
        "x_test_features = extract_features(X_test)"
      ],
      "metadata": {
        "id": "_xpEPlRQltlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Feature Scaling (NumPy):\n",
        "scaler = StandardScaler()\n",
        "x_train_features_scaled = scaler.fit_transform(x_train_features)  # Input and output are NumPy arrays\n",
        "x_test_features_scaled = scaler.transform(x_test_features)"
      ],
      "metadata": {
        "id": "tfSP_hM7l_qR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenCV : SVM with Extracted Features\n",
        "svm_features = SVC(kernel='linear', C=0.1, gamma='auto') # Updated parameters as per the GridSearchCV in previous steps\n",
        "svm_features.fit(x_train_features_scaled, y_train)"
      ],
      "metadata": {
        "id": "Wv9jLMzHmHUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate OpenCV with SVM model\n",
        "\n",
        "evaluate_model(svm_features, x_test_features_scaled, y_test, \"SVM with Features\")"
      ],
      "metadata": {
        "id": "eNO6GY4_qJST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predictions from the trained model\n",
        "y_pred = svm_features.predict(x_test_features_scaled)"
      ],
      "metadata": {
        "id": "50qliapY7H4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder # Import LabelEncoder\n",
        "# Assuming y_train contains your original training labels\n",
        "label_encoder = LabelEncoder()  # Create a LabelEncoder instance\n",
        "label_encoder.fit(y_train)  # Fit the encoder to your training labels\n",
        "\n",
        "confusionMatrix = confusion_matrix(y_test, y_pred)\n",
        "cmd = ConfusionMatrixDisplay(confusion_matrix=confusionMatrix, display_labels=label_encoder.classes_)\n",
        "plt.figure(figsize=(20, 20))\n",
        "cmd.plot()\n",
        "plt.title(\"OpenCV Confusion Matrix\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.yticks(rotation=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LrzhJD2x7C8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pillow** - Model performance evaluation using Pillow and RandomForest"
      ],
      "metadata": {
        "id": "MYlnSCvLj4m5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have a directory named 'images' with subdirectories for each class.\n",
        "\n",
        "def load_images_from_folder(folder, img_size=(224, 224), df=None): # Pass the DataFrame to the function\n",
        "    images = []\n",
        "    labels = []\n",
        "    for subdir, dirs, files in os.walk(folder):\n",
        "        for file in files:\n",
        "            # Extract image ID from filename (assuming filename format: id_*.jpg)\n",
        "            image_id = int(file.split('.')[0])\n",
        "\n",
        "            # Get the corresponding 'masterCategory' from the DataFrame\n",
        "            label = df.loc[df['id'] == image_id, 'masterCategory'].values[0]\n",
        "            image_path = os.path.join(subdir, file)\n",
        "\n",
        "            try:\n",
        "                img = Image.open(image_path).convert('RGB') # convert to RGB\n",
        "                img = img.resize((64, 64)) # resize to a consistent size\n",
        "                img_array = np.array(img).flatten() # flatten the image array\n",
        "                images.append(img_array)\n",
        "                labels.append(label)\n",
        "            except Exception as e:\n",
        "                 print(f\"Error loading image {image_path}: {e}\")\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# Load images, passing the DataFrame to get labels\n",
        "folder = \"images\"\n",
        "X, y = load_images_from_folder(folder, df=df) # Pass the DataFrame here\n",
        "\n",
        "# Perform train-test split using the new 'y'\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train an SVM classifier\n",
        "#clf = SVC(kernel='linear') # you can change the kernel\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "clf = RandomForestClassifier(max_depth=None, min_samples_split=2, n_estimators=50, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "WXzI7JWIjgbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evalauate Pillow model\n",
        "evaluate_model(clf, X_test, y_test, \"RandomForest with Pillow\")"
      ],
      "metadata": {
        "id": "o5rL-gKLsv94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
        "confusionMatrix = confusion_matrix(y_test, y_pred)\n",
        "cmd = ConfusionMatrixDisplay(confusion_matrix=confusionMatrix, display_labels=label_encoder.classes_)\n",
        "plt.figure(figsize=(20, 20))\n",
        "cmd.plot()\n",
        "plt.title(\"Pillow Confusion Matrix\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.yticks(rotation=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8Ex1N_jL6kLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**CNN** - Model performance evaluation using Convolutional Neural Network (CNN)"
      ],
      "metadata": {
        "id": "iST-pZN5tJ4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding CNN model to the problem\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Create a LabelEncoder instance\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit the encoder on the training labels and transform both training and testing labels\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# Define the CNN model\n",
        "model = keras.Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)), # Updated input shape\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(len(np.unique(y_train_encoded)), activation='softmax') # Output layer with softmax for multi-class classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',  # Use sparse categorical crossentropy for integer labels\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "X_train = X_train.reshape(-1, 64, 64, 3)  # Reshape to match the input shape of the first Conv2D layer\n",
        "X_test = X_test.reshape(-1, 64, 64, 3)  # Reshape to match the input shape of the first Conv2D layer\n",
        "\n",
        "\n",
        "# Train the model using the encoded labels\n",
        "model.fit(X_train, y_train_encoded, epochs=9, validation_data=(X_test, y_test_encoded)) # Adjust epochs as needed\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test_encoded, verbose=0)\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Make predictions using the encoded labels\n",
        "y_pred_encoded = np.argmax(model.predict(X_test), axis=1)\n",
        "\n",
        "# Inverse transform the predictions to get the original labels\n",
        "y_pred = label_encoder.inverse_transform(y_pred_encoded)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Evalauate CNN model\n",
        "evaluate_model1(model, X_test, y_test, \"CNN Model\")"
      ],
      "metadata": {
        "id": "LbVGDKd2uRVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
        "confusionMatrix = confusion_matrix(y_test, y_pred)\n",
        "cmd = ConfusionMatrixDisplay(confusion_matrix=confusionMatrix, display_labels=label_encoder.classes_)\n",
        "plt.figure(figsize=(20, 20))\n",
        "cmd.plot()\n",
        "plt.title(\"CNN Confusion Matrix\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.yticks(rotation=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-OmTh7rE4kT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Pillow package prediction is perfect with few or no false positives. The CNN model classification also has very decent prediction . See the confusion matrix for each one**"
      ],
      "metadata": {
        "id": "J9rIg4q_z17L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Update DataFrame with Predicted Label for *master category* for comparision**"
      ],
      "metadata": {
        "id": "VjXCXt0X3Y8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new column in your DataFrame for model predictions\n",
        "df['model_predicted_masterCategory'] = None  # Initialize with None\n",
        "\n",
        "# Function to predict masterCategory for images in a directory\n",
        "def predict_masterCategory(image_dir, model, df):\n",
        "    for filename in os.listdir(image_dir):\n",
        "        if filename.endswith(('.jpg', '.jpeg', '.png')):  # Check for image files\n",
        "            image_id = int(filename.split('.')[0])  # Extract image ID from filename\n",
        "            image_path = os.path.join(image_dir, filename)\n",
        "\n",
        "            try:\n",
        "                img = Image.open(image_path).convert('RGB')\n",
        "                img = img.resize((64, 64))\n",
        "                img_array = np.array(img).flatten()\n",
        "                prediction = model.predict([img_array])  # Predict using trained model\n",
        "\n",
        "                # Update the DataFrame with predictions\n",
        "                df.loc[df['id'] == image_id, 'model_predicted_masterCategory'] = prediction[0]\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing image {filename}: {e}\")\n",
        "\n",
        "# Call the function to predict and update the DataFrame\n",
        "predict_masterCategory(\"images\", clf, df)\n"
      ],
      "metadata": {
        "id": "DEXLp6VksRFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show only df with updated rows only\n",
        "df = df.dropna(subset=['model_predicted_masterCategory'])\n",
        "# Print the updated DataFrame to verify\n",
        "print(df.sample(10))"
      ],
      "metadata": {
        "id": "yl3R_tCPyihV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Plotting the results to show model accuracy. Actual vs Predicted values of Master Category**"
      ],
      "metadata": {
        "id": "DYE92MGO2v6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count occurrences of masterCategory and model_predicted_masterCategory\n",
        "master_category_counts = df['masterCategory'].value_counts().reset_index()\n",
        "predicted_master_category_counts = df['model_predicted_masterCategory'].value_counts().reset_index()\n",
        "\n",
        "# Rename columns for merging\n",
        "master_category_counts.columns = ['Category', 'Actual Count']\n",
        "predicted_master_category_counts.columns = ['Category', 'Predicted Count']\n",
        "\n",
        "# Merge the dataframes\n",
        "plot_df = pd.merge(master_category_counts, predicted_master_category_counts, on='Category', how='outer').fillna(0)\n",
        "\n",
        "# Melt the dataframe for easier plotting with seaborn\n",
        "plot_df_melted = pd.melt(plot_df, id_vars=['Category'], value_vars=['Actual Count', 'Predicted Count'], var_name='Type', value_name='Count')\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 8))\n",
        "barplot = sns.barplot(x='Category', y='Count', hue='Type', data=plot_df_melted)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.xlabel('Master Category')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Comparison of Actual and Predicted Master Category Counts')\n",
        "plt.tight_layout()\n",
        "\n",
        "# Add count annotations\n",
        "for bar in barplot.patches:\n",
        "    height = bar.get_height()\n",
        "    barplot.annotate(f'{int(height)}',\n",
        "                     xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                     xytext=(0, 5),  # Slightly above the bar\n",
        "                     textcoords='offset points',\n",
        "                     ha='center', va='bottom')  # Center and align text\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BTqiS_A32ddh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Next Steps and Future Enhancements...**\n",
        "## **Now that we created many models and identified two best models (Pillow, CNN) to classify images and label them correctly to ~99% accuracy, the Next Steps would be as follows...**\n",
        "\n",
        "## 1) Enhance the models to classify and predict labels at a detailed level like 'sub-category' or 'product name'.\n",
        "\n",
        "## 2) Create an endpoint to leverage it for use in real world application\n",
        "\n",
        "## 3 Additionally, Enhance the models to extract additional features like color, style, shape, text  during image classification\n"
      ],
      "metadata": {
        "id": "qCK9xrkGswpL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7QdBD1M8xV2H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}